# ğŸš€ CUDA 12.1 Runtimeï¼ˆå®˜æ–¹ PyTorch with Python 3.10ï¼‰
FROM pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime

# âœ… åŸºæœ¬ç’°å¢ƒè¨­å®š
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=Etc/UTC
ENV PYTHONUNBUFFERED=1

# ğŸ§© å®‰è£å¿…è¦å¥—ä»¶
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    ffmpeg \
    curl \
    libsm6 \
    libxext6 \
    tzdata \
 && rm -rf /var/lib/apt/lists/*

# âš™ï¸ è¨­å®šå·¥ä½œç›®éŒ„
WORKDIR /workspace
COPY . /workspace

# ğŸ å®‰è£ PyTorch GPU å°æ‡‰ç‰ˆæœ¬ï¼ˆCUDA 12.1ï¼‰
RUN pip install --upgrade pip setuptools wheel && \
    pip install --no-cache-dir torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121

# ğŸ“¦ å®‰è£æ¨¡å‹ä¾è³´ï¼ˆåˆ†éšæ®µé™ä½éŒ¯èª¤ç‡ï¼‰
RUN pip install --prefer-binary -r requirements.txt
RUN pip install --no-cache-dir runpod==1.4.0 requests==2.31.0

# âš¡ å»ºç«‹ FlashAttention çš„ Dummy æ¨¡çµ„
# ï¼ˆç¢ºä¿ import flash_attn.* ä¸å ±éŒ¯ï¼‰
RUN mkdir -p /workspace/flash_attn && \
    echo "def flash_attn_func(*args, **kwargs):\n    raise NotImplementedError('FlashAttention disabled. Using PyTorch native attention instead.')\n\n" > /workspace/flash_attn/__init__.py && \
    echo "def flash_attn_varlen_func(*args, **kwargs):\n    raise NotImplementedError('FlashAttention disabled.')\n\n" >> /workspace/flash_attn/__init__.py && \
    echo "def flash_attn_unpadded_func(*args, **kwargs):\n    raise NotImplementedError('FlashAttention disabled.')\n\n" >> /workspace/flash_attn/__init__.py && \
    mkdir -p /workspace/flash_attn/flash_attn_interface && \
    echo 'def flash_attn_varlen_func(*a, **k): raise NotImplementedError("FlashAttention disabled")' > /workspace/flash_attn/flash_attn_interface/__init__.py

# ğŸŒ RunPod Serverless é è¨­ä½¿ç”¨ port 5000
EXPOSE 5000

# ğŸš¦ å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
 CMD curl -f http://localhost:5000/ || exit 1

# â–¶ï¸ å•Ÿå‹• RunPod Serverless handler
CMD ["python", "handler.py"]
